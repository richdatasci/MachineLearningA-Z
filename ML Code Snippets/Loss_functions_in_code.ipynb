{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FR4Su2lzwK2F"
      },
      "outputs": [],
      "source": [
        "# mean squared error\n",
        "# The mean squared error is the squared difference\n",
        "# between the actual and the predicted values of the output. We take a\n",
        "# square of the error, as the error can be positive or negative (when the\n",
        "# predicted value is greater than the actual value and vice versa). Squaring\n",
        "# ensures that positive and negative errors do not offset each other. We\n",
        "# calculate the mean of the squared error so that the error over two different\n",
        "# datasets is comparable when the datasets are not of the same size\n",
        "\n",
        "def mse(p, y):\n",
        "  return np.mean(np.square(p - y))\n",
        "\n",
        "# Mean absolute error: The mean absolute error works in a manner that is\n",
        "# very similar to the mean squared error. The mean absolute error ensures\n",
        "# that positive and negative errors do not offset each other by taking an\n",
        "# average of the absolute difference between the actual and predicted values\n",
        "# across all data points.\n",
        "\n",
        "def mae(p, y):\n",
        "  return np.mean(np.abs(p-y))\n",
        "\n",
        "# Binary cross-entropy: Cross-entropy is a measure of the difference between\n",
        "# two different distributions: actual and predicted. Binary cross-entropy is\n",
        "# applied to binary output data, unlike the previous two loss functions that\n",
        "# we discussed (which are applied during continuous variable prediction).\n",
        "\n",
        "def binary_cross_entropy(p, y):\n",
        " return -np.mean(np.sum((y*np.log(p)+(1-y)*np.log(1-p))))\n",
        "\n",
        "# Categorical cross-entropy: Categorical cross-entropy between an array of\n",
        "# predicted values (p) and an array of actual values (y) is implemented as\n",
        "# follows:\n",
        "\n",
        "def categorical_cross_entropy(p, y):\n",
        " return -np.mean(np.sum(y*np.log(p)))\n"
      ]
    }
  ]
}